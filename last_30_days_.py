# -*- coding: utf-8 -*-
"""last 30 days .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-hR6MvheLOeaL5LBWLi_MO09Jn_mw6hm
"""

import requests
import pandas as pd
from datetime import datetime, timedelta
import time
import os

# ========== CONFIG ==========
API_KEY = "1d17e1a5e315042bcdc770c37af4ed0d"  # Replace with your actual OpenWeather API key
LAT, LON = 24.8607, 67.0011  # Karachi coordinates
CITY_ID = 1174872
CSV_PATH = "karachi_weather_hourly.csv"

# ========== FETCH FUNCTIONS ==========

def fetch_weather_at(timestamp):
    url = "http://history.openweathermap.org/data/2.5/history/city"
    params = {
        "id": CITY_ID,
        "type": "hour",
        "start": int(timestamp.timestamp()),
        "end": int((timestamp + timedelta(hours=1)).timestamp()),
        "appid": API_KEY
    }
    r = requests.get(url, params=params)
    if r.status_code == 200 and r.json().get("list"):
        return r.json()["list"][0]
    else:
        print(f"âš  Weather API error {r.status_code} at {timestamp}")
        return None

def fetch_pollution_at(timestamp):
    url = "https://api.openweathermap.org/data/2.5/air_pollution/history"
    params = {
        "lat": LAT,
        "lon": LON,
        "start": int(timestamp.timestamp()),
        "end": int((timestamp + timedelta(hours=1)).timestamp()),
        "appid": API_KEY
    }
    r = requests.get(url, params=params)
    if r.status_code == 200 and r.json().get("list"):
        return r.json()["list"][0]
    else:
        print(f"âš  Pollution API error {r.status_code} at {timestamp}")
        return None

def build_record(timestamp):
    weather = fetch_weather_at(timestamp)
    pollution = fetch_pollution_at(timestamp)
    time.sleep(1)  # avoid rate limits
    if not weather or not pollution:
        return None
    comp = pollution["components"]
    return {
        "timestamp": timestamp.strftime("%Y-%m-%d %H:%M:%S"),
        "aqi": pollution["main"]["aqi"],
        "pm2_5": comp["pm2_5"],
        "pm10": comp["pm10"],
        "temperature": round(weather["main"]["temp"] - 273.15, 2),
        "humidity": weather["main"]["humidity"],
        "wind_speed": weather["wind"]["speed"]
    }

# ========== SAVE FUNCTION ==========

def save_records(records):
    df = pd.DataFrame(records)
    if os.path.exists(CSV_PATH):
        old = pd.read_csv(CSV_PATH)
        df = pd.concat([old, df], ignore_index=True)
        df.drop_duplicates(subset=["timestamp"], inplace=True)
    df.to_csv(CSV_PATH, index=False)
    print(f"âœ… Saved {len(df)} total records to {CSV_PATH}")

# ========== MAIN FUNCTION ==========

def collect_hourly_data(start_date, end_date):
    current = start_date
    records = []
    while current < end_date:
        print(f"ðŸ“¦ Fetching {current}")
        record = build_record(current)
        if record:
            records.append(record)
        current += timedelta(hours=1)
    save_records(records)

# ========== RUN ==========

if __name__ == "__main__":
    start = datetime.now() - timedelta(days=30)
    end = datetime.now()
    collect_hourly_data(start, end)

!pip install hopsworks

!pip install "hopsworks==4.2.*"

!pip install "hopsworks[python]"

import os
os.environ["HOPSWORKS_API_KEY"] = "xcMYxvY6MzDaggfx.rCAA1m64gD8dwYmdpSRVgGJJXQj3lRzFoI8oD8MISD6tK85gjv22TtfO8eeozooQ"

import hopsworks
import pandas as pd

df = pd.read_csv("karachi_weather_hourly.csv")
df["timestamp"] = pd.to_datetime(df["timestamp"])  # âœ… Fix here

project = hopsworks.login()
fs = project.get_feature_store()

fg = fs.get_or_create_feature_group(
    name="karachi_weather_hourly",
    version=1,
    description="hourly weather and pollution data for Karachi",
    primary_key=["timestamp"],
    event_time="timestamp"
)

fg.insert(df)
print("âœ… Data inserted into Hopsworks Feature Store")

import hopsworks

project = hopsworks.login()
fs = project.get_feature_store()  # âœ… This defines 'fs'

import hopsworks

project = hopsworks.login()
fs = project.get_feature_store()

# âœ… Get the feature group
fg = fs.get_feature_group(name="karachi_weather_hourly", version=1)

# âœ… Read the data directly from Hopsworks
df = fg.select_all().read()

# âœ… Optional: sort by timestamp if needed
df = df.sort_values("timestamp").reset_index(drop=True)

print("âœ… Data fetched from Hopsworks Feature Store")

# ðŸ“¥ Fetch data from Hopsworks
fg = fs.get_feature_group("karachi_weather_hourly", version=1)
df = fg.read()

df.head(20)

df['date'] = pd.to_datetime(df['timestamp']).dt.date
df['hour'] = pd.to_datetime(df['timestamp']).dt.hour

df.drop(columns=['timestamp'], inplace=True)

df.isnull().sum()

(df['wind_speed'] == 0).sum()

num_cols = ['temperature', 'humidity', 'wind_speed', 'pm2_5', 'pm10', 'aqi']

outlier_indices = {}
for col in num_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
    outlier_indices[col] = len(outliers)

    print(f"{col}: {len(outliers)} outliers")

def cap_outliers(df, cols):
    capped_df = df.copy()

    for col in cols:

        Q1 = capped_df[col].quantile(0.25)
        Q3 = capped_df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR

        capped_df[col] = np.where(
            capped_df[col] < lower, lower,
            np.where(capped_df[col] > upper, upper, capped_df[col])
        )


    return capped_df

import numpy as np

numeric_cols = [
    "temperature", "humidity", "wind_speed", "pm2_5", "pm10", "aqi"
]

df = cap_outliers(df, numeric_cols)

df_capped = cap_outliers(df, numeric_cols)

print("Before capping:")
print(df[numeric_cols].describe())

print("\nAfter capping:")
print(df_capped[numeric_cols].describe())

def prepare_multioutput_forecast_data(df, lag_hours=72, forecast_hours=72):
    if len(df) < lag_hours + forecast_hours:
        print(f"âŒ Not enough rows. Need at least {lag_hours + forecast_hours}, got {len(df)}.")
        return None, None

    # Lag features: past 72 hours
    lag_df = pd.concat([df["aqi"].shift(i) for i in range(1, lag_hours + 1)], axis=1)
    lag_df.columns = [f"aqi_lag_{i}" for i in range(1, lag_hours + 1)]

    # Targets: next 72 hours
    target_df = pd.concat([df["aqi"].shift(-i) for i in range(1, forecast_hours + 1)], axis=1)
    target_df.columns = [f"target_t_plus_{i}" for i in range(1, forecast_hours + 1)]

    # Combine and clean
    final_df = pd.concat([lag_df, target_df], axis=1)
    final_df.dropna(inplace=True)

    X = final_df[lag_df.columns]
    y = final_df[target_df.columns]
    return X, y

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

X, y = prepare_multioutput_forecast_data(df)

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=False)

from sklearn.multioutput import MultiOutputRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

rf_model = MultiOutputRegressor(RandomForestRegressor())
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)

rf_mae = mean_absolute_error(y_test, rf_pred)
rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))
rf_r2 = r2_score(y_test, rf_pred)
rf_acc = max(0, 1 - (rf_mae / np.mean(y_test)))

print("ðŸŒ² Random Forest")
print("MAE:", round(rf_mae, 2))
print("RMSE:", round(rf_rmse, 2))
print("RÂ²:", round(rf_r2, 4))
print("Accuracy:", round(rf_acc * 100, 2), "%")

# Select last 72 AQI values from your full DataFrame
latest_lags = df["aqi"].tail(72).values.reshape(1, -1)

latest_lags_scaled = scaler.transform(latest_lags)

forecast = rf_model.predict(latest_lags_scaled)[0]  # 72 hourly AQI predictions

day1 = forecast[:24]
day2 = forecast[24:48]
day3 = forecast[48:]

print("ðŸ“… Day 1 Forecast:", day1)
print("ðŸ“… Day 2 Forecast:", day2)
print("ðŸ“… Day 3 Forecast:", day3)

import hopsworks

project = hopsworks.login()
fs = project.get_feature_store()
mr = project.get_model_registry()

import joblib

joblib.dump(rf_model, "rf_model.pkl")  # Save trained Random Forest model

!pip install hsml

import hsml
import hopsworks

project = hopsworks.login()
mr = project.get_model_registry()

from hsml.schema import Schema
from hsml.model import Model

input_example = X.iloc[0]
model_schema = Schema(X)

model = mr.python.create_model(
    name="karachi_aqi_forecaster",
    metrics={"accuracy": 0.86},
    model_schema=model_schema,
    input_example=input_example,
    description="Random Forest model for 3-day AQI hourly forecast"
)

model.save("rf_model.pkl")

